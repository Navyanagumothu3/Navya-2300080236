{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zq1luPfSo_o",
        "outputId": "bd8481a4-4dbe-4e92-d9a2-0b3d5f7e4002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Layer_Norm.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile Layer_Norm.cu\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void LayerNorm(const float* A, float* B, int rows, int cols) {\n",
        "    int row = blockIdx.x;\n",
        "\n",
        "    if (row < rows) {\n",
        "        extern __shared__ float shared[];\n",
        "        float* row_data = shared + threadIdx.x * cols;\n",
        "\n",
        "        for (int col = threadIdx.y; col < cols; col += blockDim.y) {\n",
        "            row_data[col] = A[row * cols + col];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        float mean = 0.0f;\n",
        "        for (int col = 0; col < cols; col++) {\n",
        "            mean += row_data[col];\n",
        "        }\n",
        "        mean /= cols;\n",
        "\n",
        "        float variance = 0.0f;\n",
        "        for (int col = 0; col < cols; col++) {\n",
        "            float diff = row_data[col] - mean;\n",
        "            variance += diff * diff;\n",
        "        }\n",
        "        variance /= cols;\n",
        "        float stddev = sqrtf(variance + 1e-7f);\n",
        "\n",
        "        for (int col = threadIdx.y; col < cols; col += blockDim.y) {\n",
        "            B[row * cols + col] = (row_data[col] - mean) / stddev;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int rows = 10, cols = 10;\n",
        "    float *A, *B;\n",
        "\n",
        "    A = (float*)malloc(rows * cols * sizeof(float));\n",
        "    B = (float*)malloc(rows * cols * sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            A[i * cols + j] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float *d_a, *d_b;\n",
        "    cudaMalloc(&d_a, rows * cols * sizeof(float));\n",
        "    cudaMalloc(&d_b, rows * cols * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_a, A, rows * cols * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(1, 32);\n",
        "    dim3 gridDim(rows);\n",
        "    size_t shared_memory_size = cols * sizeof(float);\n",
        "    LayerNorm<<<gridDim, blockDim, shared_memory_size>>>(d_a, d_b, rows, cols);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(B, d_b, rows * cols * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"A:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", A[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nB:\\n\");\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            printf(\"%.2f \", B[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    free(A);\n",
        "    free(B);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc Layer_Norm.cu -o Layer_Norm -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./Layer_Norm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmCW3ZNRS8mi",
        "outputId": "9ff5ff31-fd0f-4f4e-84bb-80afe04ebd56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            "0.84 0.39 0.78 0.80 0.91 0.20 0.34 0.77 0.28 0.55 \n",
            "0.48 0.63 0.36 0.51 0.95 0.92 0.64 0.72 0.14 0.61 \n",
            "0.02 0.24 0.14 0.80 0.16 0.40 0.13 0.11 1.00 0.22 \n",
            "0.51 0.84 0.61 0.30 0.64 0.52 0.49 0.97 0.29 0.77 \n",
            "0.53 0.77 0.40 0.89 0.28 0.35 0.81 0.92 0.07 0.95 \n",
            "0.53 0.09 0.19 0.66 0.89 0.35 0.06 0.02 0.46 0.06 \n",
            "0.24 0.97 0.90 0.85 0.27 0.54 0.38 0.76 0.51 0.67 \n",
            "0.53 0.04 0.44 0.93 0.93 0.72 0.28 0.74 0.64 0.35 \n",
            "0.69 0.17 0.44 0.88 0.83 0.33 0.23 0.89 0.35 0.69 \n",
            "0.96 0.59 0.66 0.86 0.44 0.92 0.40 0.81 0.68 0.91 \n",
            "\n",
            "B:\n",
            "1.01 -0.76 0.78 0.84 1.29 -1.54 -1.00 0.72 -1.22 -0.13 \n",
            "-0.51 0.15 -1.00 -0.36 1.55 1.40 0.18 0.53 -1.98 0.05 \n",
            "-0.99 -0.25 -0.60 1.57 -0.53 0.26 -0.62 -0.69 2.20 -0.33 \n",
            "-0.39 1.16 0.08 -1.43 0.20 -0.34 -0.49 1.80 -1.45 0.84 \n",
            "-0.24 0.59 -0.67 1.00 -1.06 -0.83 0.71 1.09 -1.79 1.20 \n",
            "0.69 -0.87 -0.49 1.18 1.98 0.06 -0.95 -1.10 0.45 -0.95 \n",
            "-1.48 1.45 1.18 0.97 -1.37 -0.27 -0.93 0.61 -0.38 0.24 \n",
            "-0.11 -1.91 -0.45 1.36 1.35 0.59 -1.01 0.65 0.29 -0.76 \n",
            "0.53 -1.46 -0.41 1.26 1.06 -0.83 -1.22 1.31 -0.76 0.52 \n",
            "1.22 -0.70 -0.34 0.71 -1.48 1.05 -1.70 0.48 -0.20 0.98 \n"
          ]
        }
      ]
    }
  ]
}