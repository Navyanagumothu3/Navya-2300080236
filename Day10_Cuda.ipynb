{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Pwpr_iyRlQ",
        "outputId": "6d0f6848-c416-4e76-8ca9-3ab66225be63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lash_attention_forward.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile lash_attention_forward.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <curand.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define sequence_length 64\n",
        "#define embed_dimension 64\n",
        "\n",
        "constexpr int Block_column_size = 16;\n",
        "constexpr int Block_row_size = 16;\n",
        "\n",
        "__global__ void initRandom(float* matrix, int rows, int cols, unsigned long seed) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = rows * cols;\n",
        "    if (idx < total) {\n",
        "        curandState state;\n",
        "        curand_init(seed, idx, 0, &state);\n",
        "        matrix[idx] = curand_uniform(&state);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void flashAttentionForward(\n",
        "    float* Q, float* K, float* V, float* O,\n",
        "    int seq_len, int dim)\n",
        "{\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (row < seq_len && col < dim) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < seq_len; ++i) {\n",
        "            float dot = 0.0f;\n",
        "            for (int d = 0; d < dim; ++d) {\n",
        "                dot += Q[row * dim + d] * K[i * dim + d];\n",
        "            }\n",
        "            float score = expf(dot / sqrtf((float)dim));\n",
        "            sum += score * V[i * dim + col];\n",
        "        }\n",
        "        O[row * dim + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void printMatrix(float* matrix, int rows, int cols, int printRows = 5, int printCols = 5) {\n",
        "    printf(\"Matrix sample (%dx%d of %dx%d):\\n\", printRows, printCols, rows, cols);\n",
        "    for (int i = 0; i < printRows; ++i) {\n",
        "        for (int j = 0; j < printCols; ++j) {\n",
        "            printf(\"%f \", matrix[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    size_t size = sequence_length * embed_dimension * sizeof(float);\n",
        "    float *d_Q, *d_K, *d_V, *d_O;\n",
        "    cudaMalloc(&d_Q, size);\n",
        "    cudaMalloc(&d_K, size);\n",
        "    cudaMalloc(&d_V, size);\n",
        "    cudaMalloc(&d_O, size);\n",
        "\n",
        "    dim3 blockDim(256);\n",
        "    dim3 gridDim((sequence_length * embed_dimension + 255) / 256);\n",
        "    initRandom<<<gridDim, blockDim>>>(d_Q, sequence_length, embed_dimension, 1234);\n",
        "    initRandom<<<gridDim, blockDim>>>(d_K, sequence_length, embed_dimension, 5678);\n",
        "    initRandom<<<gridDim, blockDim>>>(d_V, sequence_length, embed_dimension, 91011);\n",
        "\n",
        "    dim3 threads(Block_column_size, Block_row_size);\n",
        "    dim3 grid((embed_dimension + threads.x - 1) / threads.x,\n",
        "              (sequence_length + threads.y - 1) / threads.y);\n",
        "    flashAttentionForward<<<grid, threads>>>(d_Q, d_K, d_V, d_O, sequence_length, embed_dimension);\n",
        "\n",
        "    float* h_Q = new float[sequence_length * embed_dimension];\n",
        "    float* h_K = new float[sequence_length * embed_dimension];\n",
        "    float* h_V = new float[sequence_length * embed_dimension];\n",
        "    float* h_O = new float[sequence_length * embed_dimension];\n",
        "    cudaMemcpy(h_Q, d_Q, size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_K, d_K, size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_V, d_V, size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(h_O, d_O, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Q:\\n\"); printMatrix(h_Q, sequence_length, embed_dimension);\n",
        "    printf(\"K:\\n\"); printMatrix(h_K, sequence_length, embed_dimension);\n",
        "    printf(\"V:\\n\"); printMatrix(h_V, sequence_length, embed_dimension);\n",
        "    printf(\"O:\\n\"); printMatrix(h_O, sequence_length, embed_dimension);\n",
        "\n",
        "    delete[] h_Q;\n",
        "    delete[] h_K;\n",
        "    delete[] h_V;\n",
        "    delete[] h_O;\n",
        "    cudaFree(d_Q);\n",
        "    cudaFree(d_K);\n",
        "    cudaFree(d_V);\n",
        "    cudaFree(d_O);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc lash_attention_forward.cu -o lash_attention_forward -gencode arch=compute_75,code=sm_75 -lcurand\n",
        "!./lash_attention_forward\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1_4fhCtyb0r",
        "outputId": "1485c587-1a75-41ba-de2d-1c9aacf85985"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:\n",
            "Matrix sample (5x5 of 64x64):\n",
            "0.145468 0.820181 0.550399 0.294830 0.914733 \n",
            "0.875473 0.221577 0.295817 0.404566 0.389569 \n",
            "0.127504 0.049953 0.715364 0.101053 0.322029 \n",
            "0.375354 0.159771 0.114455 0.104466 0.069897 \n",
            "0.182689 0.044311 0.159555 0.029856 0.563860 \n",
            "K:\n",
            "Matrix sample (5x5 of 64x64):\n",
            "0.661367 0.135027 0.782970 0.186697 0.234071 \n",
            "0.911707 0.277680 0.795242 0.987023 0.082308 \n",
            "0.684712 0.102407 0.411118 0.508820 0.921537 \n",
            "0.143896 0.125743 0.863702 0.980113 0.251365 \n",
            "0.018184 0.405578 0.006400 0.798228 0.754991 \n",
            "V:\n",
            "Matrix sample (5x5 of 64x64):\n",
            "0.651542 0.707912 0.237569 0.089901 0.431445 \n",
            "0.977253 0.048838 0.813942 0.426914 0.397408 \n",
            "0.845735 0.783821 0.840378 0.296731 0.079394 \n",
            "0.196971 0.339382 0.245803 0.880243 0.291420 \n",
            "0.702024 0.153379 0.898005 0.907008 0.050992 \n",
            "O:\n",
            "Matrix sample (5x5 of 64x64):\n",
            "261.446136 213.799515 253.103088 228.763107 213.048355 \n",
            "259.997131 213.284042 252.750275 230.118103 213.506989 \n",
            "238.396255 196.676163 235.457123 214.321701 201.330933 \n",
            "227.966705 187.504120 218.737381 197.067001 187.501190 \n",
            "239.148148 196.843353 233.618607 208.953979 199.157684 \n"
          ]
        }
      ]
    }
  ]
}